# Problem responses

1. If the tokenizers are different, we would first have one set of tokenized inputs for each model. When generating a new token, we would first generate `v_head` for the larger model (which for any reasonable `alpha` will be sparse). For each token in `v_head` we would then decode it with the expert tokenizer into text then re-encode the text again with the amateur tokenizer. We could then compute `p_ama` for each candidate token and compute the contrastive loss much as before. After selecting a next token, we would update the amateur input sequence with its respective tokenizer. If the expert and amateur tokenizers are very different, the same text may produce a different number of tokens. If the amateur tokenizer decodes some candidate `x` in `v_head` as multiple tokens `x_1, ..., x_n`, I would compute `p_ama(x)` by "teacher-forcing" the generation of `x_1, ..., x_n` on the amateur model, and computing the log likelihood at every step letting us get `p_ama(x) = p_ama(x_1) * ... * p_ama(x_n)`. If the expert tokenizer produces more finegrained tokens than the amateur tokenizer, the `p_ama(x)` may be out of distribution from the true contrastive difference we want to compute: we could resolve this by sampling n-grams for `v_head` if we knew something about the tokenizers before hand. 
2. I don't think contrastive decoding is used in practice because of the increased computation cost (also I haven't read many papers about it...). It may be the case that the amateur model is smaller and therefore faster for compute (comparatively low overhead), but for efficient inference systems I/O costs are significant. It is possible to paralelize over multiple GPUs but this further adds a memory footprint (GPUs are expensive!) and additional synchronization costs. If the tokenizers don't match, the retokenization scheme described above happens on CPU and leads to costly CPU syncs. Experimentally, running the contrastive decoding model did not clearly outperform just the expert alone, especially not for the increased inference time. I also noticed that the contrastive decoding approach was biased like the smaller model to longer generations (although I am not certain why). There is, as mentioned in the paper, the issue of false positives/negatives. Intuitively, we are assuming that the amateur model captures the distributions of failure modes well but that's not exactly what its trained for. Perhaps, for more complex reasoning tasks, distancing from a smaller model could be powerful, but I'm not sure how useful this is across broad domains especially for fixed hyperparmeters alpha and tau.

# Other notes
All of my experiments and most of the written code is in the notebook run on a T4 via Google Colab. Locally, I have a Mac M3 which has been behaving a little weirdly in main.py. After implementing the basic decoding approach from the paper, I tried a few optimizations in the notebook.

* First, I tried adding a KV cache for each model which turbo charged long sequence (~100s of tokens) generation as expected. We went from 0.67 sec per token to 0.07 seconds. However, the KV cached model behaved quite differently (qualitatively worse) than the base model which I suspect is because of accumulating biases? I did not have the time to look into this fully in the allocated 2 hrs, so I just added a flag to toggle the caching.
* Second, I noticed that running the small model takes 0.17 seconds per token, and the expert model takes 0.40 seconds per token. This means there was a 0.67 - 0.57 = 0.10 second overhead in contrastive decoding. This is because we were explicitly setting -inf scores to all words in the vocabulary when we only need to consider values in `v_head` which is sparse. We instead just computed the log probabilities at the selected indices directly which reduced seconds per token to a reasonable 0.57. It also did not change generation. 